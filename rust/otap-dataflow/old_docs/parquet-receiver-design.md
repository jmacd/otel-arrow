# Parquet Receiver Design Document

## Overview

**üéØ Hackathon Project**: This is a hackathon implementation focused on demonstrating the concept of parquet-to-OTAP reconstruction. Priority is on getting a working demo quickly, not production-ready code.

This document outlines the design and implementation plan for a Parquet Receiver component that can read parquet files generated by the existing Parquet Exporter and reconstruct OTAP data for downstream processing.

## Problem Statement

The OTAP dataflow project has a parquet exporter that writes telemetry data to parquet files in a denormalized star-schema format. However, there is no corresponding receiver component that can read these parquet files and feed them back into the pipeline. **Primary Hackathon Goal**: **Data replay and demonstration** - Show that we can successfully read parquet files generated by the exporter and reconstruct valid OTAP data for pipeline processing.

*Future production uses could include*: Batch processing, data migration, and analytics integration.

## Parquet Exporter Structure Analysis

### File Organization

The parquet exporter creates a star-schema layout with the following structure:

```
base_directory/
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ _part_id=<uuid1>/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ part-<timestamp>-<uuid>.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ part-<timestamp>-<uuid>.parquet
‚îÇ   ‚îî‚îÄ‚îÄ _part_id=<uuid2>/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ log_attrs/
‚îÇ   ‚îú‚îÄ‚îÄ _part_id=<uuid1>/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ part-<timestamp>-<uuid>.parquet
‚îÇ   ‚îî‚îÄ‚îÄ _part_id=<uuid2>/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ resource_attrs/
‚îÇ   ‚îî‚îÄ‚îÄ ... (similar structure)
‚îî‚îÄ‚îÄ scope_attrs/
    ‚îî‚îÄ‚îÄ ... (similar structure)
```

### Key Characteristics

1. **Partitioning**: Files are partitioned by `_part_id` (UUID) which ensures IDs are unique within each partition
2. **Time-based naming**: Files are named with timestamps indicating when the first batch was received
3. **Star schema**: Each OTAP payload type gets its own directory/table
4. **Relationships**: Tables are joined using `id -> parent_id` relationships within the same partition

### Signal Types Supported

- **Logs**: `logs/`, `log_attrs/`, `resource_attrs/`, `scope_attrs/`
- **Traces**: `spans/`, `span_attrs/`, `span_events/`, `span_event_attrs/`, `span_links/`, `span_link_attrs/`, `resource_attrs/`, `scope_attrs/`
- **Metrics**: `univariate_metrics/`, `number_data_points/`, `number_dp_attrs/`, `histogram_data_points/`, `histogram_dp_attrs/`, etc.

## Reconstruction Algorithm

Based on the coworker's pseudocode, the reconstruction process follows this pattern:

```rust
for path in watch_for_new_files_in_dir("/tmp/logs") {
    let part_id = extract_part_id_from_path(&path);
    let file = extract_filename(&path);

    // Read main signal data
    let logs = read_parquet_file(&path);

    // Extract IDs for related queries
    let parent_ids = extract_ids_from_logs(&logs);
    let scope_ids = extract_scope_ids_from_logs(&logs);
    let resource_ids = extract_resource_ids_from_logs(&logs);

    // Query related tables
    let log_attrs = query_table("log_attrs", part_id, parent_ids);
    let scope_attrs = query_table("scope_attrs", part_id, scope_ids);
    let resource_attrs = query_table("resource_attrs", part_id, resource_ids);

    // Reconstruct OTAP batch
    let otap_batch = reconstruct_otap_logs(logs, log_attrs, scope_attrs, resource_attrs);
}
```

## Architecture Design

### Component Structure

```
ParquetReceiver
‚îú‚îÄ‚îÄ Configuration
‚îÇ   ‚îú‚îÄ‚îÄ base_uri: String (path to parquet root directory)
‚îÇ   ‚îú‚îÄ‚îÄ signal_types: Vec<SignalType> (logs, traces, metrics)
‚îÇ   ‚îú‚îÄ‚îÄ polling_interval: Duration (how often to scan for new files)
‚îÇ   ‚îî‚îÄ‚îÄ batch_size: usize (optional batch size limit)
‚îú‚îÄ‚îÄ FileWatcher
‚îÇ   ‚îú‚îÄ‚îÄ Monitor main signal directories for new files
‚îÇ   ‚îú‚îÄ‚îÄ Track processed files to avoid duplicates
‚îÇ   ‚îî‚îÄ‚îÄ Sort files by timestamp for ordered processing
‚îú‚îÄ‚îÄ QueryEngine
‚îÇ   ‚îú‚îÄ‚îÄ Read parquet files using Apache Arrow
‚îÇ   ‚îú‚îÄ‚îÄ Execute multi-table queries to join related data
‚îÇ   ‚îî‚îÄ‚îÄ Handle missing or incomplete related tables gracefully
‚îî‚îÄ‚îÄ OTAPReconstructor
    ‚îú‚îÄ‚îÄ Convert Arrow RecordBatches to OTAP format
    ‚îú‚îÄ‚îÄ Rebuild proper parent-child relationships
    ‚îî‚îÄ‚îÄ Generate OtapPdata messages for pipeline
```

### Receiver Type

- **Local Receiver**: File I/O operations don't benefit from shared state across cores
- **Non-blocking**: Use async file operations to avoid blocking the runtime
- **Streaming**: Process files as streams rather than loading entirely into memory

### File Processing Strategy

1. **Discovery**: Periodically scan main signal directories (logs/, spans/, univariate_metrics/)
2. **Ordering**: Process files in timestamp order (extracted from filename)
3. **Deduplication**: Maintain state of processed files to avoid reprocessing
4. **Guaranteed completeness**: Related attribute files are guaranteed to exist when main files appear
5. **Error Handling**: Skip corrupted files and log errors, continue processing others

## Implementation Challenges and Solutions

### 1. File Timing and Atomicity

**Challenge**: Files might be detected while still being written by the exporter.

**Solution**: 
- **Preferred**: Modify the parquet exporter to use atomic rename (write to temp file, then rename to final location)
- **Fallback**: Add a minimum file age before processing (e.g., 5 seconds) if atomic rename isn't implemented
- Check file size stability as additional safety measure
- Note: Atomic rename ensures files only appear when complete, eliminating the race condition entirely

### 2. Missing Related Tables

**Challenge**: Main table files might exist before their related attribute files are written.

**Solution**:
- **Guaranteed ordering**: The parquet exporter writes attribute tables (children) before main signal tables (parents)
- **Simplified approach**: When a main signal file appears, all related attribute files are guaranteed to exist
- **No retry needed**: Process main signal files immediately upon detection
- **Validation**: Optionally verify related files exist for debugging, but they should always be present

### 3. Memory Management

**Challenge**: Large parquet files could consume significant memory.

**Solution**:
- **DataFusion-first approach**: Leverage DataFusion's built-in memory management and streaming
- **Query optimization**: Use DataFusion's query planner for efficient execution plans
- **Streaming results**: DataFusion provides streaming RecordBatch iterators by default
- **Memory limits**: Configure DataFusion's memory pool limits for bounded resource usage
- **Pushdown optimizations**: DataFusion automatically applies predicate and projection pushdown

### 4. Cross-Partition Queries

**Challenge**: Related data is distributed across partition directories.

**Solution**:
- Query all partitions for a given time window
- Use DataFusion for efficient cross-partition queries
- Cache partition metadata for faster lookups

## DataFusion Integration Strategy for OTAP Parquet Reconstruction

### **Key Architecture Decisions:**

1. **SessionContext per Operation**: Create a new `SessionContext` for each parquet file reconstruction to avoid table name conflicts and ensure clean state.

2. **Multi-Table Registration Pattern**: For reconstructing `OtapArrowRecords::Logs`, we need to register 4 tables:
   - `logs` (main signal table)  
   - `log_attrs` (log attributes)
   - `resource_attrs` (resource attributes)
   - `scope_attrs` (scope attributes)

3. **Partition-Aware Querying**: Since parquet files are organized by `_part_id=<uuid>/`, we'll:
   - Extract part_id from file paths
   - Register related tables from the same partition
   - Use SQL JOINs to reconstruct relationships via `id -> parent_id` links

4. **DataFusion Query Pattern**:
   ```rust
   // Register tables for a partition
   ctx.register_parquet("logs", "/tmp/output_parquet_files/logs/_part_id=<uuid>/*.parquet", options).await?;
   ctx.register_parquet("log_attrs", "/tmp/output_parquet_files/log_attrs/_part_id=<uuid>/*.parquet", options).await?;
   // ... other tables

   // Execute reconstruction query
   let df = ctx.sql(r#"
       SELECT 
           logs.id,
           logs.timestamp_unix_nano,
           logs.body,
           -- collect related attributes
       FROM logs
       LEFT JOIN log_attrs ON logs.id = log_attrs.parent_id
       LEFT JOIN resource_attrs ON logs.resource.id = resource_attrs.parent_id
       LEFT JOIN scope_attrs ON logs.scope.id = scope_attrs.parent_id
       ORDER BY logs.id
   "#).await?;
   ```

5. **RecordBatch to OtapArrowRecords Conversion**: 
   - Use DataFusion's `collect()` to get `Vec<RecordBatch>`
   - Transform each `RecordBatch` back to the original OTAP schema format
   - Apply the reverse of `decode_transport_optimized_ids()` to restore proper ID relationships

### **File Discovery Strategy:**

The design document mentions that **attribute files are guaranteed to exist when main signal files appear** because the parquet exporter writes children before parents. This simplifies our discovery logic:

1. **Watch Main Signal Directories**: Monitor `/tmp/output_parquet_files/logs/` for new files
2. **Extract Part ID**: Parse `_part_id=<uuid>` from file paths  
3. **Immediate Processing**: When a new logs file appears, all related attribute files are guaranteed to exist
4. **Atomic Processing**: Process one complete partition at a time

### **Error Handling Strategy:**

1. **Partition Isolation**: If one partition fails to reconstruct, continue processing others
2. **Schema Flexibility**: Use DataFusion's schema inference to handle schema evolution
3. **Missing Data Graceful Handling**: Use LEFT JOINs so missing attribute data doesn't break reconstruction

### **Memory Management:**

1. **DataFusion's Built-in Streaming**: DataFusion handles memory management and provides streaming `RecordBatch` iterators
2. **Bounded Resource Usage**: Configure DataFusion's memory pool limits
3. **Query Optimization**: DataFusion automatically applies predicate and projection pushdown

This strategy leverages DataFusion's strengths:
- ‚úÖ **Proven parquet reading** with automatic schema inference
- ‚úÖ **SQL query optimization** for multi-table joins  
- ‚úÖ **Memory management** with streaming execution
- ‚úÖ **Schema flexibility** to handle evolution
- ‚úÖ **Performance optimizations** like pushdown predicates

## Dependencies and Technology Stack

### Core Dependencies
- `datafusion` - Primary query engine with built-in parquet support, memory management, and streaming
- `arrow` - Arrow format support (included with DataFusion)
- `parquet` - Parquet file format support (included with DataFusion)
- `tokio` - Async runtime for non-blocking I/O
- `notify` or periodic scanning - File system monitoring

### Integration Points
- `otap_df_engine::local::receiver::Receiver` trait implementation
- `OtapPdata` from `crate::pdata` for pipeline integration
- `OtapArrowRecords` for OTAP format reconstruction

## Configuration Schema

```yaml
nodes:
  parquet_receiver:
    kind: receiver
    plugin_urn: "urn:otel:otap:parquet:receiver"
    config:
      base_uri: "./parquet_data"
      signal_types: ["logs", "traces", "metrics"]
      polling_interval: "5s"
      processing_options:
        batch_size: 1000
        min_file_age: "10s"  # Only needed if atomic rename not implemented
        validate_relations: false  # Optional validation for debugging
```

## Implementation Plan

### Phase 0: DataFusion Architecture Study ‚úÖ **COMPLETED**

1. Study DataFusion's SessionContext and DataFrame API ‚úÖ
2. Understand DataFusion's memory management and streaming execution ‚úÖ
3. Learn table registration patterns for partitioned parquet datasets ‚úÖ
4. Investigate SQL vs DataFrame API for multi-table joins ‚úÖ
5. Test DataFusion with existing generated parquet data ‚úÖ
6. Document optimal DataFusion configuration for our use case ‚úÖ

### Phase 1: Hackathon MVP - Logs Reconstruction üéØ

1. Create simple `ParquetQueryEngine` module using DataFusion
2. Implement basic table registration for one partition at a time
3. Build SQL query to reconstruct logs from 4 tables (logs, log_attrs, resource_attrs, scope_attrs)
4. Quick test with `/tmp/output_parquet_files/` to validate basic functionality

### Phase 2: OTAP Integration

1. Create minimal conversion from DataFusion RecordBatch to `OtapArrowRecords::Logs`
2. Handle basic ID relationships (don't worry about transport optimization decoding)
3. Generate `OtapPdata` that can be sent through pipeline

### Phase 3: Simple File Discovery

1. Basic directory scanning (no watching, just one-time scan)
2. Process first available partition for demo
3. Minimal error handling (log and skip bad files)

### Phase 4: Demo Integration

1. Create basic `ParquetReceiver` that implements the receiver trait
2. Minimal configuration (just base_path)
3. Register in receiver factory for demo

### Phase 5: Demo Validation ‚ú®

1. **End-to-end test**: Read parquet ‚Üí reconstruct OTAP ‚Üí process in pipeline
2. **Demo script**: Show logs being replayed from parquet files
3. **Visual validation**: Compare original vs reconstructed data in pipeline output

## Hackathon Scope Decisions

**‚úÖ Simplified for Demo**:
1. **File Locking**: Skip - assume files are complete when discovered
2. **Checkpointing**: Skip - process files fresh each run
3. **Partition Pruning**: Skip - process all available partitions
4. **Schema Evolution**: Skip - assume stable schema from exporter
5. **Compression**: Use whatever DataFusion supports by default
6. **Error Recovery**: Log errors and continue, don't crash
7. **Performance**: Optimize for correctness and demo-ability, not throughput

## Hackathon Success Criteria üèÜ

1. **‚úÖ Basic Functionality**: Can read parquet files and produce valid `OtapArrowRecords::Logs`
2. **‚úÖ Pipeline Integration**: Reconstructed data flows through existing pipeline components
3. **‚úÖ Demo-able**: Can show logs being replayed from stored parquet files
4. **‚úÖ Proof of Concept**: Demonstrates the feasibility of parquet ‚Üí OTAP reconstruction

*Nice to have*: Support for traces and metrics signal types, but logs are sufficient for the demo.

## Next Steps

## Implementation Status and Next Steps

Based on this analysis, we have completed Phase 0 (DataFusion Architecture Study) and now understand DataFusion's capabilities and optimal usage patterns for our partitioned parquet datasets. The DataFusion-first approach leverages battle-tested query optimization, memory management, and streaming capabilities rather than attempting to build these complex features ourselves.

**üöÄ Hackathon Status**: Ready to build MVP - Focus on getting logs reconstruction working end-to-end!

**Key Hackathon Decisions**:

- ‚úÖ Use DataFusion for simple parquet reading (proven, fast to implement)
- ‚úÖ Start with logs only (4 tables: logs, log_attrs, resource_attrs, scope_attrs)
- ‚úÖ Process one partition at a time (simplest approach)
- ‚úÖ Skip production concerns (locking, checkpointing, performance optimization)
- ‚úÖ Focus on correctness and demo-ability over scalability

**Ready to Code**: `/tmp/output_parquet_files/` has real test data. Let's build the MVP! üî®

**Estimated Development Time**: 2-4 hours for basic logs reconstruction + pipeline integration.
