# OTAP Sampling Receiver - Project Overview

**Status**: ðŸŽ¯ **READY FOR IMPLEMENTATION**  
**Date**: September 18, 2025  
**Project Phase**: Architecture & Implementation Planning

## Executive Summary

This document outlines the implementation plan for the **OTAP Sampling Receiver** - a sophisticated, query-driven component that reads parquet files generated by the OTAP parquet exporter and performs configurable sampling operations using DataFusion as the query engine.

Building on the successful **parquet_receiver proof of concept**, this new component represents a production-ready evolution that combines:

- **Query-First Architecture**: Flexible, SQL-driven data processing instead of hard-coded logic
- **Temporal Windowing**: Time-aligned processing with safety margins for data completeness  
- **Weighted Sampling**: Advanced statistical sampling using custom UDAFs (User-Defined Aggregate Functions)
- **Arrow Compute Optimization**: Vectorized operations for maximum performance

## Background & Motivation

### Lessons from the Parquet Receiver Proof of Concept

The initial **parquet_receiver** successfully demonstrated:

- âœ… **DataFusion Integration**: Reading partitioned parquet files and reconstructing OTAP data
- âœ… **Schema Understanding**: Proper handling of OTAP's star-schema structure
- âœ… **Pipeline Integration**: Seamless integration with existing OTAP pipeline components
- âœ… **Arrow Performance**: Efficient RecordBatch processing and transformation

### Key Learnings & Improvements

From the parquet_receiver experience, we identified several optimization opportunities:

1. **Hard-coded Logic Limitation**: The original receiver used "SELECT * FROM logs" approach with fixed streaming joins
2. **Performance Bottlenecks**: Element-by-element processing instead of vectorized Arrow compute operations
3. **Limited Flexibility**: No support for sampling, filtering, or complex queries
4. **Memory Inefficiency**: Full column processing when only subsets are needed

## Project Objectives

### Primary Goals

1. **Flexible Query Processing**: Support any SQL query, from 100% pass-through to complex weighted sampling
2. **Production Performance**: Leverage Arrow compute kernels for 10-100x performance improvements
3. **Temporal Processing**: Process time-aligned windows with configurable safety margins
4. **Statistical Sampling**: Implement weighted reservoir sampling with proper representativity handling

### Success Criteria

- [ ] **100% Compatibility**: Process same parquet files as the proof-of-concept receiver
- [ ] **Performance Improvement**: 10x faster processing for typical workloads through vectorized operations
- [ ] **Query Flexibility**: Support both pass-through (100% sampling) and complex sampling queries
- [ ] **Production Ready**: Robust error handling, monitoring, and operational features

## Architecture Overview

### Core Design Principles

1. **Query-First Processing**: Every operation starts with a SQL query against OTAP tables
2. **Temporal Windows**: Process time-aligned chunks with safety margins for completeness
3. **Vectorized Operations**: Use Arrow compute kernels instead of element-by-element processing
4. **Flexible Sampling**: Support everything from pass-through to weighted reservoir sampling

### Component Architecture

```text
Sampling Receiver
â”œâ”€â”€ Temporal Window Manager
â”‚   â”œâ”€â”€ File aging and safety margin handling
â”‚   â”œâ”€â”€ Time-aligned window assembly (1-minute, 5-minute, etc.)
â”‚   â””â”€â”€ Partition discovery across time windows
â”œâ”€â”€ DataFusion Query Engine  
â”‚   â”œâ”€â”€ 4 Partitioned Table Registration (logs, log_attributes, resource_attributes, scope_attributes)
â”‚   â”œâ”€â”€ Virtual partition columns (_part_id) for automatic partition handling
â”‚   â”œâ”€â”€ Custom UDAF registration (weighted_reservoir_sample)
â”‚   â””â”€â”€ Query execution with predicate pushdown
â”œâ”€â”€ Arrow Compute Optimization Layer
â”‚   â”œâ”€â”€ Vectorized ID column transformations (UInt32 â†’ UInt16 with normalization)
â”‚   â”œâ”€â”€ Efficient array slicing (process 1K from 100K without full column materialization)
â”‚   â”œâ”€â”€ View type materialization (UTF8View â†’ UTF8, BinaryView â†’ Binary)
â”‚   â””â”€â”€ Memory-efficient batch processing
â””â”€â”€ OTAP Reconstruction Engine
    â”œâ”€â”€ Query result processing (log_attributes as primary output)
    â”œâ”€â”€ Related table filtering (logs, resource_attributes, scope_attributes)
    â”œâ”€â”€ Streaming merge coordination
    â””â”€â”€ OtapArrowRecords generation
```

### Data Flow Architecture

```text
Parquet Files â†’ Temporal Windows â†’ DataFusion Query â†’ Arrow Optimization â†’ OTAP Output
      â†‘                â†‘                 â†‘                    â†‘                 â†‘
  Partitioned      Time-aligned      SQL-driven         Vectorized         Streaming
   star-schema      processing        flexibility        operations          records
```

## Technical Foundation

### DataFusion Integration Strategy

**4 Partitioned Tables with Virtual Columns**:
Instead of registering NÃ—4 individual tables, we use **4 partitioned ListingTables** with automatic partition column injection:

```rust
// âœ… 4 table registrations (one per OTAP table type) with partition columns
for table_type in ["logs", "log_attributes", "resource_attributes", "scope_attributes"] {
    let listing_options = ListingOptions::new(Arc::new(ParquetFormat::default()))
        .with_table_partition_cols(vec![
            ("_part_id".to_string(), DataType::Utf8),  // Virtual column
        ]);
    
    ctx.register_table(table_type, Arc::new(ListingTable::try_new(config)))?;
}
```

}

**Benefits**:

- **4 registrations** instead of NÃ—4 registrations  
- **Virtual `_part_id` column** automatically populated by DataFusion
- **Automatic predicate pushdown** on partition columns
- **Natural JOINs** between OTAP table types

### Query Types Supported

#### 1. **Pass-Through Query (100% sampling)**

```sql
-- âœ… Simple query against partitioned log_attributes table
-- âœ… DataFusion: Automatic predicate pushdown and partition pruning
SELECT *
FROM log_attributes
WHERE timestamp_unix_nano >= {window_start_ns}
  AND timestamp_unix_nano < {window_end_ns}
ORDER BY _part_id, parent_id, key;
```

#### 2. **Filtered Query with Temporal Boundaries**

```sql
-- âœ… Clean JOIN between partitioned tables with automatic partition alignment
SELECT la.*
FROM log_attributes la
JOIN logs l ON l.id = la.parent_id AND l._part_id = la._part_id
WHERE la.timestamp_unix_nano >= {window_start_ns}
  AND la.timestamp_unix_nano < {window_end_ns}
  AND l.severity_number >= 17  -- Error and above
ORDER BY la._part_id, la.parent_id, la.key;
```

#### 3. **Weighted Sampling Query**

```sql
-- âœ… Advanced sampling with custom UDAF
WITH service_weights AS (
    SELECT 
        l.id, l._part_id,
        ra.str as service_name,
        COALESCE(CAST(la.str AS DOUBLE), 1.0) as input_weight
    FROM logs l
    JOIN resource_attributes ra ON ra.parent_id = l.id AND ra._part_id = l._part_id
    LEFT JOIN log_attributes la ON la.parent_id = l.id AND la._part_id = l._part_id 
                                 AND la.key = 'sampling.adjusted_count'
    WHERE l.timestamp_unix_nano >= {window_start_ns} AND l.timestamp_unix_nano < {window_end_ns}
),
sampling_decisions AS (
    SELECT service_name, _part_id,
           weighted_reservoir_sample(STRUCT(id, input_weight), 100) AS sample_decisions
    FROM service_weights GROUP BY service_name, _part_id
)
-- Process sampling results and reconstruct log_attributes with adjusted weights
SELECT /* ... comprehensive log_attributes reconstruction ... */ ORDER BY _part_id, parent_id, key;
```

### Arrow Compute Optimization

**Performance Strategy**: Replace element-by-element processing with vectorized Arrow compute kernels for **10-100x performance improvements**.

#### Current Bottleneck (from parquet_receiver)

```rust
// ðŸŒ SLOW: Element-by-element processing with builders
for i in 0..uint32_array.len() {
    if uint32_array.is_null(i) {
        uint16_builder.append_null();
    } else {
        let original_id = uint32_array.value(i);
        let normalized_id = self.normalize_id(original_id)?;
        uint16_builder.append_value(normalized_id);
    }
}
```

#### Optimized Approach (for new receiver)

```rust
// âœ… FAST: Vectorized operations with Arrow compute kernels
use arrow::compute::kernels::numeric::sub_wrapping;
use arrow::compute::kernels::cast::cast;

fn transform_id_column_vectorized(&self, column: &ArrayRef) -> Result<ArrayRef> {
    // Step 1: Zero-copy slice to get only the data we need
    let column_slice = column.slice(start_row, num_rows);
    
    // Step 2: Vectorized subtraction - SIMD optimized
    let offset_scalar = UInt32Array::new_scalar(self.batch_start_id);
    let normalized_u32 = sub_wrapping(&column_slice, &offset_scalar)?;
    
    // Step 3: Vectorized type conversion - also SIMD optimized  
    let normalized_u16 = cast(&normalized_u32, &DataType::UInt16)?;
    
    Ok(normalized_u16)
}
```

}

**Expected Performance Gains**:

- **ID Transform**: 10-100x faster (SIMD utilization + no builder allocations)
- **Column Slicing**: 50x faster (zero-copy slicing vs full cloning)
- **View Materialization**: 10x faster (slice-first approach)
- **Memory Allocation**: Linear â†’ Constant (vectorized operations)

## Phased Implementation Strategy

## Implementation Strategy

### Phase 1: Foundation & Core DataFusion Integration

**Duration**: 1-2 weeks  
**Priority**: High

**Objectives**:

- Create the basic DataFusion-powered query engine
- Implement 4 partitioned table registration with virtual columns
- Build temporal window management system
- Test basic pass-through queries

**Key Components**:

1. **SamplingReceiver struct** implementing the Receiver trait
2. **TemporalWindowManager** for time-aligned processing
3. **DataFusionQueryEngine** with 4-table registration
4. **Basic configuration system** for query specification

**Deliverables**:

- [ ] Basic SamplingReceiver can process parquet files using DataFusion
- [ ] Pass-through queries work identically to parquet_receiver
- [ ] Temporal windowing processes time-aligned chunks
- [ ] Configuration supports query specification

### Phase 2: Arrow Compute Optimization

**Duration**: 1-2 weeks  
**Priority**: High (performance critical)

**Objectives**:

- Replace element-by-element processing with vectorized operations
- Implement efficient column slicing and array management
- Optimize memory usage patterns
- Achieve 10x performance improvement targets

**Key Components**:

1. **VectorizedIDTransformer** using Arrow compute kernels
2. **EfficientArraySlicer** for zero-copy operations
3. **ViewTypeMaterializer** with slice-first optimization
4. **MemoryEfficientBatchProcessor**

**Deliverables**:

- [ ] Vectorized ID transformations (UInt32â†’UInt16 + normalization)
- [ ] Efficient column slicing (1K from 100K without full materialization)
- [ ] View type optimizations (UTF8Viewâ†’UTF8, BinaryViewâ†’Binary)
- [ ] Performance benchmarks showing 10x improvement

### Phase 3: Weighted Sampling UDAF Implementation

**Duration**: 1-2 weeks  
**Priority**: Medium (feature expansion)

**Objectives**:

- Implement the weighted_reservoir_sample UDAF
- Create comprehensive sampling query templates
- Add support for multiple sampling strategies
- Validate statistical properties of sampling

**Key Components**:

1. **WeightedReservoirSampleUDAF** with proper accumulator
2. **SamplingQueryTemplates** for different use cases
3. **StatisticalValidation** testing framework
4. **AdjustedCountAttributeGeneration**

**Deliverables**:

- [ ] Working weighted_reservoir_sample UDAF registered with DataFusion
- [ ] Sampling queries produce statistically valid results
- [ ] Adjusted count attributes properly inserted into log_attributes
- [ ] End-to-end sampling workflow functional

### Phase 4: Production Hardening & Monitoring

**Duration**: 1 week  
**Priority**: Medium (operational readiness)

**Objectives**:

- Add comprehensive error handling and recovery
- Implement monitoring and observability features  
- Create operational documentation and runbooks
- Performance optimization and memory profiling

**Key Components**:

1. **RobustErrorHandling** with retry logic
2. **MetricsAndMonitoring** integration
3. **ConfigurationValidation** and documentation
4. **MemoryProfiler** and optimization

**Deliverables**:

- [ ] Comprehensive error handling with graceful degradation
- [ ] Monitoring metrics for operations team
- [ ] Production-ready configuration options
- [ ] Performance profiling and optimization

## Configuration Schema

### Basic Configuration

```yaml
nodes:
  sampling_receiver:
    kind: receiver  
    plugin_urn: "urn:otel:otap:sampling:receiver"
    config:
      # Data source configuration
      base_uri: "/path/to/parquet/files"
      signal_types: ["logs"]  # Future: traces, metrics
      
      # Temporal processing configuration
      temporal:
        window_granularity: "1m"        # Process 1-minute aligned windows
        processing_delay: "10m"         # Wait 10 minutes before processing (safety margin)
        max_clock_drift: "1m"           # Account for clock differences
        max_file_duration: "1m"         # Maximum time span per file
      
      # Query configuration - defaults to 100% pass-through
      query: |
        SELECT *
        FROM log_attributes
        WHERE timestamp_unix_nano >= {window_start_ns}
          AND timestamp_unix_nano < {window_end_ns}
        ORDER BY _part_id, parent_id, key
      
      # Performance tuning
      performance:
        batch_size: 1000
        enable_arrow_optimization: true
        memory_limit: "1GB"
```

### Advanced Sampling Configuration

```yaml
nodes:
  sampling_receiver:
    config:
      # ... basic config ...
      
      # Advanced weighted sampling query
      query: |
        WITH service_weights AS (
          SELECT 
            l.id, l._part_id,
            ra.str as service_name,
            COALESCE(CAST(la.str AS DOUBLE), 1.0) as input_weight
          FROM logs l
          JOIN resource_attributes ra ON ra.parent_id = l.id AND ra._part_id = l._part_id AND ra.key = 'service.name'
          LEFT JOIN log_attributes la ON la.parent_id = l.id AND la._part_id = l._part_id AND la.key = 'sampling.adjusted_count'
          WHERE l.timestamp_unix_nano >= {window_start_ns} AND l.timestamp_unix_nano < {window_end_ns}
        ),
        sampling_decisions AS (
          SELECT service_name, _part_id,
                 weighted_reservoir_sample(STRUCT(id, input_weight), 100) AS sample_decisions
          FROM service_weights GROUP BY service_name, _part_id
        )
        -- ... complete sampling query ...
        
      # Sampling-specific configuration
      sampling:
        sample_size: 100          # K parameter for reservoir sampling
        preserve_weights: true    # Generate sampling.adjusted_count attributes
        strategy: "weighted"      # weighted, uniform, stratified
```

## Risk Assessment & Mitigation

### High Priority Risks ðŸš¨

#### 1. Memory Pressure from Array Slicing

**Risk**: Zero-copy slices keep original large batches in memory longer

**Mitigation**:

- Implement streaming patterns (process â†’ send â†’ drop)
- Monitor memory usage with configurable limits
- Use DataFusion's memory pools for bounded resource usage

#### 2. Query Performance with Complex Sampling

**Risk**: Weighted sampling queries may be slow on large datasets  

**Mitigation**:

- Implement query plan analysis and optimization
- Use DataFusion's built-in predicate pushdown
- Consider query result caching for repeated time windows

### Medium Priority Risks âš ï¸

#### 3. **DataFusion Schema Evolution**
**Risk**: Parquet schema changes could break queries
**Mitigation**:
- Implement schema validation and compatibility checking
- Use flexible query generation that adapts to available columns
- Create schema migration tooling

#### 4. Temporal Window Edge Cases

**Risk**: Files spanning multiple time windows or late arrivals

**Mitigation**:

- Implement configurable safety margins  
- Add duplicate detection across windows
- Create manual reprocessing capabilities for edge cases

### Low Priority Risks âœ…

#### 5. **UDAF Integration Complexity**
**Risk**: Custom UDAFs may have integration issues with DataFusion
**Mitigation**: 
- Comprehensive unit testing of UDAF logic
- Fallback to simpler sampling approaches if needed
- Extensive integration testing with real data

## Success Criteria & Validation

### Functional Requirements

- [ ] **100% Compatibility**: Process same parquet files as parquet_receiver proof-of-concept
- [ ] **Query Flexibility**: Support both pass-through and complex sampling queries  
- [ ] **Temporal Processing**: Handle time-aligned windows with safety margins
- [ ] **Statistical Validity**: Sampling produces unbiased, representative results

### Performance Requirements

- [ ] **10x Performance**: Arrow optimization achieves 10x improvement for typical workloads
- [ ] **Memory Efficiency**: Process large datasets within configurable memory limits
- [ ] **Throughput**: Match or exceed parquet_receiver throughput with added functionality

### Operational Requirements

- [ ] **Production Ready**: Robust error handling, monitoring, and operational features
- [ ] **Configuration Flexibility**: Support multiple deployment scenarios
- [ ] **Documentation**: Comprehensive setup, configuration, and troubleshooting guides

## Next Steps & Immediate Actions

### Week 1: Project Setup & Foundation

1. **Repository Structure**: Create new module for sampling receiver
2. **Basic DataFusion Integration**: Implement 4-table registration with virtual columns  
3. **Temporal Window Manager**: Basic time-aligned processing
4. **Configuration System**: Support for configurable queries

### Week 2: Core Query Engine

1. **Pass-through Queries**: Implement and test 100% sampling  
2. **Filtered Queries**: Add support for WHERE clauses and JOINs
3. **Integration Testing**: Validate against parquet_receiver test data
4. **Performance Baseline**: Establish current performance metrics

### Week 3: Arrow Optimization Layer

1. **Vectorized ID Transformations**: Implement Arrow compute-based operations
2. **Efficient Array Slicing**: Zero-copy operations for sub-batch processing  
3. **View Type Optimization**: Slice-first materialization
4. **Performance Validation**: Measure 10x improvement targets

### Week 4: UDAF Implementation

1. **WeightedReservoirSample UDAF**: Core sampling logic
2. **Sampling Query Templates**: Complete weighted sampling query
3. **Statistical Validation**: Verify sampling correctness
4. **End-to-end Testing**: Full sampling workflow

---

## Conclusion

The **OTAP Sampling Receiver** represents a significant evolution from the parquet_receiver proof of concept, incorporating lessons learned and advanced optimization techniques. By combining DataFusion's query capabilities with Arrow compute optimizations and flexible sampling strategies, this component will provide a robust, high-performance foundation for advanced OTAP data processing.

The phased implementation approach ensures steady progress toward production readiness while maintaining the flexibility to adapt to emerging requirements and optimization opportunities.

**ðŸš€ Ready to Begin Implementation - Let's build the future of OTAP data processing!**
